{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM language model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPbX6YdDuHdBVBrW0UIKtR5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedElgabryi/LSTM-for-Language-Modeling/blob/master/LSTM_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DypqgFzeSekI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from blocks import initialization\n",
        "from blocks.bricks import Linear, NDimensionalSoftmax, Tanh\n",
        "from blocks.bricks.parallel import Fork\n",
        "from blocks.bricks.recurrent import GatedRecurrent, LSTM, SimpleRecurrent\n",
        "from blocks.bricks.lookup import LookupTable\n",
        "\n",
        "\n",
        "def initialize(to_init):\n",
        "    for bricks in to_init:\n",
        "        bricks.weights_init = initialization.Uniform(width=0.08)\n",
        "        bricks.biases_init = initialization.Constant(0)\n",
        "        bricks.initialize()\n",
        "\n",
        "\n",
        "def softmax_layer(h, y, vocab_size, hidden_size):\n",
        "    hidden_to_output = Linear(name='hidden_to_output', input_dim=hidden_size,\n",
        "                              output_dim=vocab_size)\n",
        "    initialize([hidden_to_output])\n",
        "    linear_output = hidden_to_output.apply(h)\n",
        "    linear_output.name = 'linear_output'\n",
        "    softmax = NDimensionalSoftmax()\n",
        "    y_hat = softmax.apply(linear_output, extra_ndim=1)\n",
        "    y_hat.name = 'y_hat'\n",
        "    cost = softmax.categorical_cross_entropy(\n",
        "        y, linear_output, extra_ndim=1).mean()\n",
        "    cost.name = 'cost'\n",
        "    return y_hat, cost\n",
        "\n",
        "\n",
        "def rnn_layer(dim, h, n):\n",
        "    linear = Linear(input_dim=dim, output_dim=dim, name='linear' + str(n))\n",
        "    rnn = SimpleRecurrent(dim=dim, activation=Tanh(), name='rnn' + str(n))\n",
        "    initialize([linear, rnn])\n",
        "    return rnn.apply(linear.apply(h))\n",
        "\n",
        "\n",
        "def gru_layer(dim, h, n):\n",
        "    fork = Fork(output_names=['linear' + str(n), 'gates' + str(n)],\n",
        "                name='fork' + str(n), input_dim=dim, output_dims=[dim, dim * 2])\n",
        "    gru = GatedRecurrent(dim=dim, name='gru' + str(n))\n",
        "    initialize([fork, gru])\n",
        "    linear, gates = fork.apply(h)\n",
        "    return gru.apply(linear, gates)\n",
        "\n",
        "\n",
        "def lstm_layer(dim, h, n):\n",
        "    linear = Linear(input_dim=dim, output_dim=dim * 4, name='linear' + str(n))\n",
        "    lstm = LSTM(dim=dim, name='lstm' + str(n))\n",
        "    initialize([linear, lstm])\n",
        "    return lstm.apply(linear.apply(h))\n",
        "\n",
        "\n",
        "def nn_fprop(x, y, vocab_size, hidden_size, num_layers, model):\n",
        "    lookup = LookupTable(length=vocab_size, dim=hidden_size)\n",
        "    h = lookup.apply(x)\n",
        "    cells = []\n",
        "    for i in range(num_layers):\n",
        "        if model == 'rnn':\n",
        "            h = rnn_layer(hidden_size, h, i)\n",
        "        if model == 'gru':\n",
        "            h = gru_layer(hidden_size, h, i)\n",
        "        if model == 'lstm':\n",
        "            h, c = lstm_layer(hidden_size, h, i)\n",
        "            cells.append(c)\n",
        "    return softmax_layer(h, y, vocab_size, hidden_size) + (cells, )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}